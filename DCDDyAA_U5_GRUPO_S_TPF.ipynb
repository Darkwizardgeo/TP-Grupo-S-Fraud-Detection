{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff700d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib pandas seaborn scikit-learn dtale seaborn xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabbd57",
   "metadata": {},
   "source": [
    "- ## 0 **Import de librerias a utilizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"./src\"))\n",
    "\n",
    "import dtale\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from utils import fraud_licit_ratio, calcular_nulos, split_by_suffix, nan_ratio, visualize_missing_values, missing_audit\n",
    "from pipelines import split_features , logistic_pipeline_v2, svm_pipeline_v2, xgboost_pipeline_v2\n",
    "\n",
    "from evaluate import evaluate_multiple\n",
    "from feature_engineering import build_preprocessor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09040e89",
   "metadata": {},
   "source": [
    "- ### 0.1 Import de Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_transaction = pd.read_csv('./dataset/train_transaction.csv')\n",
    "df_train_identity = pd.read_csv('./dataset/train_identity.csv')\n",
    "df_test_transaction = pd.read_csv('./dataset/test_transaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afacc26",
   "metadata": {},
   "source": [
    "- ## 1 **Preparacion inicial de los datasets**\n",
    "\n",
    "> Nuestros datasets esta dividido en dos partes, uno tiene mucha informacion de identidad y otro la data de la transaccion. No todas las transacciones tienen su correspondiente en el Dataset Identity asi que hacemos un merge outer para mantener todos los datos en un dataset grande.\n",
    "\n",
    "> Utilizamos una herramienta para analisis EDA rapido para visualizar y navegar un dataset que tiene mas de 400 columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8feab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_transaction = pd.merge(\n",
    "    df_train_transaction,\n",
    "    df_train_identity,\n",
    "    on=\"TransactionID\",\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "print(\"Shape merged Outer:\", df_full_transaction.shape)\n",
    "dtale.show(df_full_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84781b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "calcular_nulos(df_full_transaction)\n",
    "fraud_licit_ratio(df_full_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_ratio(df_train_transaction, 'Transanciones')\n",
    "\n",
    "nan_ratio(df_train_identity, 'Entidades')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66170071",
   "metadata": {},
   "source": [
    "> En nuestra evaluacion vemos que el Dataset luego de unidos los datos de transacciones y de identidades, tienen muchos datos NaN, nuestro aproach inicial fue de reducir las columnas con alto nive de % NaN y luego limpiar las filas. Esto nos dejaba con un dataset 40% aproximadamente mas chico y nos preocupaba que nuestro modelo se viera afectado en la prediccion por esto.\n",
    "\n",
    "> Decidimos tomar el camina de auditar las columnas con un metodo denominado `lift missing values` que nos permite determinar la viabilidad de eliminar una columna evaluando que tanto impacto tiene en nuestro target. Si es cercana a 1 la variable podria se elminada, si es mayor o menor el valor NaN o la presencia de la columna con valor no NaN podria darnos pista de un fraude y ser valiosa.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ad8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, drop_candidates = missing_audit(df_full_transaction, 'isFraud')\n",
    "print(result)\n",
    "print(\"Columnas candidatas a eliminar:\", drop_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7167e",
   "metadata": {},
   "source": [
    "- ## 2 Feature engineering y data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721933c0",
   "metadata": {},
   "source": [
    "> En nuestro archivo Pipelines las funciones v2 actualmente hacer un feature engineering llamando a la clase feature_engineering con el metodo build_preprocessor, trabajamos en especial de evitar la multicolinealidad para modelos como la Regresion Logistica y Imputamos valores NaN con la mediana en columnas numericas entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8f8ec",
   "metadata": {},
   "source": [
    "- ## 3 Preparacion de Variables para entrenamiento de Modelos \n",
    "\n",
    "> Nuetra intencion es realizar un entrenamiento de modelos para\n",
    "- Regresion Logistica\n",
    "- SVM\n",
    "- XGBoost\n",
    "\n",
    "> Para esto necesitamos que nuestro dataset este preparado para cada uno de nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a20dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos fijamos nuestro target y separamos features categoricas y numericas\n",
    "y = df_full_transaction['isFraud']\n",
    "X = df_full_transaction.drop(columns=['isFraud'])\n",
    "\n",
    "cat_cols, num_cols = split_features(X)\n",
    "print(f\"Categorical columns: {len(cat_cols)}\")\n",
    "print(f\"Numerical columns: {len(num_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85260121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'Logistic Regression': logistic_pipeline_v2(df_full_transaction),\n",
    "    'SVM': svm_pipeline_v2(df_full_transaction),\n",
    "    'XGBoost': xgboost_pipeline_v2(df_full_transaction)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43095d10",
   "metadata": {},
   "source": [
    "- ## 4 Entrenamiento de Modelos\n",
    "\n",
    "> Entrenamos nuestros modelos aplicando la estrategia de StratifiedKFold, para evaluar nuestras metricas deseadas con nuestros pipelines en un KFold de 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub, _, y_sub, _ = train_test_split(X, y, \n",
    "                                      train_size=0.2,  # usar 20% de datos\n",
    "                                      stratify=y,\n",
    "                                      random_state=42)\n",
    "results = evaluate_multiple(pipelines, X_sub, y_sub, cv_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d220ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_multiple(pipelines, X, y, cv_splits=5)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpfinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
